{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"An example-based DepthAI ROS2 driver","text":""},{"location":"#you-can-find-newer-version-of-the-driver-at-httpsgithubcomluxonisdepthai-ros","title":"You can find newer version of the driver at https://github.com/luxonis/depthai-ros","text":""},{"location":"#remarks","title":"Remarks","text":"<p>if you encounter static link issue, try removing hunder cache sudo rm -r /home/{user_name}/.hunter/</p>"},{"location":"#deploy-with-docker","title":"deploy with docker","text":"<pre><code># run docker build at root folder\n ./build_deploy_dockerfile.sh\n\n # docker run\ndocker run -it --rm -v /dev:/dev --privileged --network=\"host\" novelte/depthai {your_launch_file}\n\n # docker launch rgbd_camera_modi.launch.py\ndocker run -it --rm -v /dev:/dev --privileged --network=\"host\" novelte/depthai ros2 launch depthai_ros_driver rgbd_camera_modi.launch.py\n</code></pre> <p>Work presented here has been developed by Adam Serafin while at Inmotion Labs</p> <p>Hi! This is a simple (for now) project that enables Luxonis' DepthAI camera to work with ROS2 based systems. In its basic form it provides depth, rgb, mono camera outputs, as well as recording .h264 streams. For now, following examples can be run: * Mobilenet detection example * Segmentation example * Multi camera launch  * OAK 1 launch (it needs to set depth stream parameters to false) * RGBD camera (pretty similar to how Realsense works) * Rtabmap launch example * Stella slam (former OpenVSLAM) example Code was being tested on:  - OAK-D lite camera on a x86 PC - ROS2 Galactic  - OAK-D camera on a x86 PC - ROS2 Galactic  - OAK-D camera on an ARM based system - Nvidia Jetson, using ROS2 Galactic built from source  - OAK-D camera on Raspberry Pi using Ubuntu Core 22.04 and ROS2 Humble</p> <p></p>"},{"location":"#known-issues","title":"Known issues","text":"<ul> <li>mobilenet &amp; segmentation camera experiences lower frame rate than what is set.</li> <li><code>description.launch.py</code> is currently only a dummy file, it should be replaced with a more concise urdf-based launch in the future. For now, you can edit it to get you camera links in a proper way.</li> <li>depth parameters are tuned for better accuracy, and depth stream itself might be lagging a little bit</li> <li>you should use best USB cable available</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#one-line-setup","title":"One line setup:","text":"<p>To just download the whole workspace, install dependencies and build the code, you can run: Note This assumes that you have already installed specific ROS2 distro.</p> <pre><code>wget https://raw.githubusercontent.com/Serafadam/depthai_ros_driver/galactic/get_full_workspace.sh -O - | bash -s galactic\n</code></pre> <p>You can also specify which ROS2 distro this will downlad deps for, just change <code>bash -s galactic</code> to, for example <code>bash -s foxy</code></p> <p>When using the Remote Containers plugin, after the container builds, run <code>setup all</code> task from VSCode. This automatically pulls the <code>depthai-core</code> git submodule and uses rosdep to install all ROS dependencies. If you want to install it outside of the container, check out <code>.vscode/tasks.json</code> for the exact commands used. You can build both <code>depthai-core</code> and <code>depthai-ros-driver</code> by using colcon in the base directory.</p>"},{"location":"#running","title":"Running","text":"<ul> <li> <p>To launch it together with rviz and a node that outputs the detections as TF frames and markers, <code>ros2 launch depthai_ros_driver mobilenet_camera.launch.py use_rviz:=True</code></p> </li> <li> <p>In <code>multicamera.launch.py</code>, you can edit the file to provide mx_ids for specific cameras.</p> </li> </ul>"},{"location":"#slam","title":"SLAM","text":""},{"location":"#rtabmap","title":"Rtabmap","text":"<p>First install rtabmap_ros package <code>sudo apt install ros-distro-rtabmap-ros</code></p>"},{"location":"#stella-slam-former-openvslam","title":"Stella Slam (former OpenVSLAM)","text":"<p>Use <code>setup_stella_slam.sh</code> to install requirements (or follow the script manually). An example config file is provided in <code>config</code> directory</p>"},{"location":"#developing","title":"Developing","text":"<p>If you want to develop a specific camera system, you should inherit the <code>BaseCamera</code> class. If you want to disable some streams, just set one of the respective base parameters to false.</p>"},{"location":"#parameters","title":"Parameters","text":"<p>Parameters are split into two groups, one with prefix <code>i_</code> and the other one with prefix <code>r_</code> * <code>i_</code> parameters are set initially, and they cannot be changed without restarting the camera * <code>r_</code> parameters can be changed at runtime, using for example RQT configuration tool All of these parameters can also be overriden programatically, see <code>mobilenet_camera_obj</code> example.</p> <p>List of parameters: Base parameters:  * i_usb_speed  * i_max_q_size  * i_enable_rgb  * i_enable_depth  * i_enable_lr  * i_enable_imu  * i_enable_recording  * i_enable_logging  * i_camera_mxid  * i_camera_ip</p> <p>RGB parameters: * i_rgb_fps  * i_preview_size  * i_rgb_width * i_rgb_resolution * i_set_isp * i_inverleaved  * i_keep_preview_aspect_ratio * r_set_man_focus * r_man_focus * r_set_man_exposure * r_rgb_exposure * r_rgb_iso * r_set_man_whitebalance * r_whitebalance</p> <p>Stereo parameters: * i_mono_fps * i_mono_resolution * i_align_depth * i_lr_check * i_lrc_threshold * i_depth_filter_size * i_stereo_depth_threshold * i_subpixel * i_extended_disp * i_rectify_edge_fill_color * i_enable_speckle_filter * i_speckle_range * i_enable_temporal_filter * i_enable_spatial_filter * i_hole_filling_radius * i_spatial_filter_iterations * i_threshold_filter_min_range * i_threshold_filter_max_range * i_decimation_factor</p>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li>Update topics - [x]</li> <li>Make the Node Components - [x]</li> <li>Add more example launch files to choose from, such as rgb-only or stereo only cameras - [x]</li> <li>Reorganize the code - [x]</li> <li>Add example how to use Camera Calibration - [x]</li> <li>Visual Slam example - [x]</li> <li>Add docker images - [x]</li> <li>Add a script to get all camera ids [ ]</li> <li>More configuration options [ ]</li> <li>More examples:</li> <li>Gaze estimation [ ]</li> <li>Hand detection [ ]</li> <li>Tracking [ ]</li> <li>Separate publishing parameters [ ]</li> <li>Refactor (as always)</li> </ul>"},{"location":"LICENSE/","title":"LICENSE","text":"<p>MIT License</p> <p>Copyright (c) 2021 Adam Serafin</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"}]}